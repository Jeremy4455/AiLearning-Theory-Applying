# 第四章——多头注意力机制——QK矩阵相乘

<img src="../assets/image-20240502141958851.png" alt="image-20240502141958851" style="zoom:50%;" />

### 前言

上一章，我们已经研究了矩阵相乘以及QK相乘的过程，接下来我们完整的走一遍多头注意力机制里的流程。





### QK矩阵相乘

上面我们计算好了QK相乘后的矩阵，我们看下原文中的Attention公式
$$
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$
<img src="../assets/image-20240502140356134.png" alt="image-20240502140356134" style="zoom:50%;" />

我们单独拿1个批次的第一个头出来

![image-20240502140715615](../assets/image-20240502140715615.png)

第一行的所有数据，分别上`LL`分别跟`LLM with me.郭同学热爱AI喜欢游戏`每个词的相关性。第二行则是`M`分别跟`LLM with me.郭同学热爱AI喜欢游戏`每个词的相关性。越高则代表两个字的相关性越高，越低则代表两个字的相关性越低。

<img src="../assets/image-20240502141342857.png" alt="image-20240502141342857" style="zoom:50%;" />



### Scale缩放

缩放层。缩放操作通常用于调整矩阵乘法的结果。在计算注意力分数之前，会将查询和键的矩阵乘法结果除以一个缩放因子，通常是键向量维度的平方根。可以看到上面的它的公式就是让QK矩阵去除以根号下的d_k，而d_k就是每个头的维度，即768除以12个头，得到64，d_k=64。

也就是下面这个表的每一个值都会除以64，相当于值会进行缩小。

<img src="../assets/image-20240502142816044.png" alt="image-20240502142816044" style="zoom:50%;" />



### Mask
