# 第三章——位置编码

<img src="../assets/image-20240421205946626.png" alt="文字向量化" style="zoom: 50%;" />

### 前言

第二章我们讲了文字向量化，而在向量化传到多头注意力之前，还有一个位置信息编码。

WHY：在自然语言处理中，单词的顺序对于理解文本的含义至关重要。例如，"狗咬人"和"人咬狗"词汇相同但意义不同。然而，Transformer模型的核心机制——自注意力（self-attention）本身并不具备捕捉序列顺序的能力。它允许模型在处理一个单词时考虑到句子中的所有其他单词，但它不会区分这些单词的位置关系。这就是为什么位置编码至关重要的原因：它们为模型提供了单词在句子中的位置信息，使得模型能够理解词序和语法结构。

WHAT：位置编码是一种向模型输入的每个单词嵌入向量中添加信息的技术，以便模型能够识别单词的位置。



### 位置编码是怎么算的？

在《Attention is all you need》中，位置编码是有正弦和余弦函数计算出来的，且是固定的。

优点：

- 由于是固定公式，不需要通过训练来学习，可以直接计算出任意位置的编码。
- 具有可能的泛化能力，理论上可以处理比训练时见过的序列更长的序列。
- 由于正弦和余弦函数是周期性的，这种编码方式可能帮助模型捕捉到某种周期性的模式。

GPT-2的位置编码是可学习的参数。这意味着模型在训练过程中会调整位置编码，以便更好地适应特定任务的需求。

优点：

- 由于是通过训练学习的，模型可以自适应地为特定任务找到最佳的位置编码。
- 可以捕捉到数据中的特定模式，可能比固定公式更适合某些任务。
- 由于是模型的一部分，位置编码可以与其他模型参数一起进行端到端的优化。



### Transformer里的位置编码方法（原文）

~~~python
import numpy as np

def get_positional_encoding(max_seq_len, d_model):
    position_enc = np.array([
        [pos / np.power(10000, 2 * (j // 2) / d_model) for j in range(d_model)]
        if pos != 0 else np.zeros(d_model)
        for pos in range(max_seq_len)
    ])
    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i
    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1
    return torch.from_numpy(position_enc).type(torch.FloatTensor)

# 假设我们的模型维度是768，最大序列长度是1024
max_seq_len = 1024
d_model = 768
positional_encoding = get_positional_encoding(max_seq_len, d_model)
print(positional_encoding)
"""out:
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.4147e-01,  5.4030e-01,  8.2843e-01,  ...,  1.0000e+00,
          1.0243e-04,  1.0000e+00],
        [ 9.0930e-01, -4.1615e-01,  9.2799e-01,  ...,  1.0000e+00,
          2.0486e-04,  1.0000e+00],
        ...
"""
~~~

<img src="../assets/image-20240427180449855.png" alt="image-20240427180449855" style="zoom:50%;" />

> 我们上面参数也参考GPT-2的，比如max_seq_len=1024、d_model=768

你也可以根据第二章的"LLM with me"的索引，去获取对应的位置编码。

~~~python
print(positional_encoding[13][:10])
print(positional_encoding[14][:10])
print(positional_encoding[11][:10])
"""out:
tensor([ 0.4202,  0.9074,  0.1252,  0.9921, -0.1744,  0.9847, -0.4519,  0.8921,
        -0.6858,  0.7278])
tensor([0.9906, 0.1367, 0.8920, 0.4520, 0.7018, 0.7124, 0.4454, 0.8953, 0.1523,
        0.9883])
tensor([-1.0000,  0.0044, -0.9673, -0.2535, -0.8724, -0.4889, -0.7253, -0.6884,
        -0.5387, -0.8425])
"""
~~~

<img src="../assets/image-20240427180954246.png" alt="image-20240427180954246" style="zoom:50%;" />

为什么是用正弦和余弦函数，对于正弦函（sin）：最大值是 1，最小值是 -1。对于余弦函数（cos）：最大值是 1，最小值是 -1。也就是它们可以保证值是比较小的，而且也是符合深度学习模型可学习的参数。

其中最重要的是允许模型学习相对位置：由于正弦和余弦函数的周期性，对于任意固定偏移量 `k`，`PE(pos+k)`可以表示为 `PE(pos)` 的线性函数。这意味着模型可以很容易地通过学习注意力权重来关注相对位置，因为相对位置的编码可以通过简单的线性变换来获得。

~~~markdown
原文：
We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).
~~~



我们用更简单的语言来解释这个概念：

~~~markdown
想象一下，你有一串彩色的灯泡，每个灯泡都有不同的颜色。这些颜色按照一定的模式重复，比如红、绿、蓝、红、绿、蓝，依此类推。如果你知道了这个模式，即使你被蒙上眼睛，只要告诉你起点的颜色，你也能猜出后面第几个灯泡是什么颜色。
在Transformer模型中，我们想要模型能够理解单词在句子中的位置。为此，我们给每个位置分配了一个特殊的标记，就像给每个灯泡分配了一种颜色。正弦和余弦函数就像是这些颜色的模式，它们以一种规律的方式重复。
现在，当模型看到一个单词和它的位置标记时，它可以通过这个位置标记（就像颜色模式）来理解这个单词与句子中其他单词的相对位置。因为正弦和余弦函数是周期性的，所以模型可以通过简单的数学运算（线性变换）来预测单词之间的相对位置，就像你可以通过颜色模式来预测下一个灯泡的颜色一样。
这种方法的好处是，模型不仅知道每个单词的绝对位置（就像知道每个灯泡的编号），而且还能理解单词之间的相对位置（就像知道一个灯泡与另一个灯泡之间有几个灯泡）。这对于理解和生成语言非常重要，因为在语言中，单词的意义往往取决于它们在句子中的位置和它们与其他单词的关系。
~~~

关键词：编号和可预测的颜色顺序



### GPT-2的位置编码方法



### 向量加法

以GPT-2为例（Hugging Face开源的GPT），