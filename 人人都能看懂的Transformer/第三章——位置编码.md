# 第三章——位置编码

<img src="../assets/image-20240421205946626.png" alt="文字向量化" style="zoom: 50%;" />

### 前言

第二章我们讲了文字向量化，而在向量化传到多头注意力之前，还有一个位置信息编码。

WHY：在自然语言处理中，单词的顺序对于理解文本的含义至关重要。例如，"狗咬人"和"人咬狗"词汇相同但意义不同。然而，Transformer模型的核心机制——自注意力（self-attention）本身并不具备捕捉序列顺序的能力。它允许模型在处理一个单词时考虑到句子中的所有其他单词，但它不会区分这些单词的位置关系。这就是为什么位置编码至关重要的原因：它们为模型提供了单词在句子中的位置信息，使得模型能够理解词序和语法结构。

WHAT：位置编码是一种向模型输入的每个单词嵌入向量中添加信息的技术，以便模型能够识别单词的位置。



### 位置编码是怎么算的？

在《Attention is all you need》中，位置编码是有正弦和余弦函数计算出来的，且是固定的。

优点：

- 由于是固定公式，不需要通过训练来学习，可以直接计算出任意位置的编码。
- 具有可能的泛化能力，理论上可以处理比训练时见过的序列更长的序列。
- 由于正弦和余弦函数是周期性的，这种编码方式可能帮助模型捕捉到某种周期性的模式。

GPT-2的位置编码是可学习的参数。这意味着模型在训练过程中会调整位置编码，以便更好地适应特定任务的需求。

优点：

- 由于是通过训练学习的，模型可以自适应地为特定任务找到最佳的位置编码。
- 可以捕捉到数据中的特定模式，可能比固定公式更适合某些任务。
- 由于是模型的一部分，位置编码可以与其他模型参数一起进行端到端的优化。



### Transformer里的原始方法





### 向量加法

以GPT-2为例（Hugging Face开源的GPT），